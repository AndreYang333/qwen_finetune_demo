# configs/train_config.yaml
model_name_or_path: Qwen/Qwen-1_8B-Chat
lora: true  # true = LoRA 微调, false = 全量微调
output_dir: ./output_qwen

max_seq_length: 1024
train_batch_size: 1
eval_batch_size: 1
learning_rate: 2e-5
num_train_epochs: 3
report_to: "tensorboard"    
logging_dir: "./logs"  
logging_steps: 10
save_steps: 200
eval_steps: 200
warmup_ratio: 0.03

lora_config:
  r: 8
  lora_alpha: 16
  lora_dropout: 0.05
  bias: "none"
  task_type: "CAUSAL_LM"
  target_modules:
    - c_attn
    - c_proj
    - w1
    - w2

data:
  train_file: ./data/train.jsonl
  val_file: null
